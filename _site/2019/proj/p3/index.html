<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Buildings built in minutes - An SfM Approach</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="Course materials and notes for University of Maryland's class CMSC733: Computer Vision.">
    <link rel="canonical" href="http://cmsc733.github.io/2019/proj/p3/">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">

    <!-- Google fonts -->
    <link href='http://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>

    <!-- Google tracking -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-46895817-2', 'auto');
      ga('send', 'pageview');

    </script>
    
</head>


    <body>

    <header class="site-header">

  <div class="wrap title-wrap">
    <a class="site-title" href="/">CMSC733 Computer Vision</a>
  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Buildings built in minutes - An SfM Approach</h1>
  </header>

  <article class="post-content">
  <p><strong>To be submitted in a group of two.</strong></p>

<p>Table of Contents:<br />
- <a href="#due">1. Deadline</a><br />
- <a href="#intro">2. Introduction</a><br />
- <a href="#trad">3. Phase 1: Traditional Approach</a><br />
	- <a href="#featmatch">3.1. Feature Matching</a><br />
	- <a href="#estfundmatrix">3.2. Estimating Fundamental Matrix</a>  <br />
		- <a href="#epipole">3.2.1. Epipolar Geometry</a><br />
		- <a href="#fundmatrix">3.2.2. Fundamental Matrix</a><br />
		- <a href="#ransac">3.2.3. Match Outlier Rejection using RANSAC</a></p>

<div class="highlighter-rouge"><pre class="highlight"><code>- [3.3. Estimate *Essential Matrix* from Fundamental Matrix](#estE)

- [3.4. Estimate Camera Pose from Essential Matrix](#essential)

- [3.5. Check for Cheirality Condition using Triangulation](#tri)
	- [3.5.1. Non-Linear Triangulation](#nonlintri)
- [3.6. Perspective-$$n$$-points](#pnp)
	- [3.6.1. Linear Camera Pose Estimation](#campose)
	- [3.6.2. PnP RANSAC](#pnpransac)
	- [3.6.3. NonLinear PnP](#nonpnp)
- [3.7. Bundle Adjustment](#ba)
	- [3.7.1. Visibility Matrix](#vismatrix)
	- [3.7.2. Bundle Adjustment](#sba)
</code></pre>
</div>

<ul>
  <li>
    <p><a href="#combine">4. Putting the pipeline together</a></p>
  </li>
  <li>
    <p><a href="#dataset">5. Notes about Data Set</a></p>
  </li>
  <li>
    <p><a href="#sub">6. Submission Guidelines</a></p>

    <ul>
      <li>
        <p><a href="#files">6.1. File tree and naming</a></p>
      </li>
      <li>
        <p><a href="#report">6.2. Report</a></p>
      </li>
    </ul>
  </li>
  <li>
    <p><a href="#coll">7. Collaboration Policy</a></p>
  </li>
</ul>

<p><a name="due"></a><br />
## 1. Deadline <br />
<strong>11:59PM, Thursday, April 22, 2019.</strong></p>

<p><a name="intro"></a><br />
## 2. Introduction<br />
We have been playing with images for so long, mostly in 2D scene. Recall <a href="/2019/proj/p1">project 1</a> where we stitched multiple images with about 30-50% common features between a couple of images. Now let’s learn how to <strong>reconstruct a 3D scene and simultaneously obtain the camera poses</strong> of a monocular camera w.r.t. the given scene. This procedure is known as Structure from Motion (SfM). As the name suggests, you are creating the entire <strong>rigid</strong> structure from a set of images with different view points (or equivalently a camera in motion). A few years ago, Agarwal et. al published <a href="http://grail.cs.washington.edu/rome/rome_paper.pdf">Building Rome in a Day</a> in which they reconstructed the entire city just by using a large collection of photos from the Internet. Ever heard of Microsoft <a href="https://en.wikipedia.org/wiki/Photosynth">Photosynth?</a> <em>Facinating? isn’t it!?</em> There are a few open source SfM algorithm available online like <a href="http://ccwu.me/vsfm/">VisualSFM</a>. <em>Try them!</em></p>

<p>Let’s learn how to recreate such algorithm. There are a few steps that collectively form SfM:</p>

<ul>
  <li><strong>Feature Matching</strong> and Outlier rejection using <strong>RANSAC</strong></li>
  <li>Estimating <strong>Fundamental Matrix</strong></li>
  <li>Estimating <strong>Essential Matrix</strong> from Fundamental Matrix</li>
  <li>Estimate <strong>Camera Pose</strong> from Essential Matrix</li>
  <li>Check for <strong>Cheirality Condition</strong> using <strong>Triangulation</strong></li>
  <li><strong>Perspective-n-Point</strong></li>
  <li><strong>Bundle Adjustment</strong></li>
</ul>

<p><a name="intradtro"></a><br />
## 3. Traditional Approach to the SfM problem</p>

<p><a name="featmatch"></a><br />
### 3.1. Feature Matching, Fundamental Matrix and RANSAC<br />
We have already learned about keypoint matching using SIFT keypoints and descriptors (Recall Project 1: Panorama Stitching). It is important to refine the matches by rejecting outline correspondence. Before rejecting the correspondences, let us first understand what Fundamental matrix is!</p>

<div class="fig fighighlight">
  <img src="/assets/2019/p3/featmatch.png" width="100%" />
  <div class="figcaption">
    Figure 1: Feature matching between two images from different views.
  </div>
  <div style="clear:both;"></div>
</div>

<p><a name="estfundmatrix"></a><br />
### 3.2. Estimating Fundamental Matrix <br />
The fundamental matrix, denoted by <script type="math/tex">F</script>, is a <script type="math/tex">3\times 3</script> (<em>rank 2</em>) matrix that relates the corresponding set of points in two images from different views (or stereo images). But in order to understand what fundamental matrix actually is, we need to understand what <em>epipolar geometry</em> is! The epipolar geometry is the intrinsic projective geometry between two views. It only depends on the cameras’ internal parameters (<script type="math/tex">K</script> matrix) and the relative pose <em>i.e.</em> it is <strong>independent of the scene structure</strong>.</p>

<p><a name="epipole"></a><br />
#### 3.2.1. Epipolar Geometry<br />
Let’s say a point <script type="math/tex">\mathbf{X}</script> in the 3D-space (viewed in two images) is captured as <script type="math/tex">\mathbf{x}</script> in the first image and <script type="math/tex">\mathbf{x'}</script> in the second. <em>Can you think how to formulate the relation between the corresponding image points <script type="math/tex">\mathbf{x}</script> and <script type="math/tex">\mathbf{x'}</script>?</em> Consider Fig. 2. Let <script type="math/tex">\mathbf{C}</script> and <script type="math/tex">\mathbf{C'}</script> be the respective camera centers which forms the baseline for the stereo system. Clearly, the points <script type="math/tex">\mathbf{x}</script>, <script type="math/tex">\mathbf{x'}</script> and <script type="math/tex">\mathbf{X}</script> (or <script type="math/tex">\mathbf{C}</script>, <script type="math/tex">\mathbf{C'}</script> and <script type="math/tex">\mathbf{X}</script>) are coplanar <em>i.e.</em>  <script type="math/tex">\mathbf{\overrightarrow{Cx}}\cdot \left(\mathbf{\overrightarrow{CC'}}\times\mathbf{\overrightarrow{C'x'}}\right)=0</script> <br />
and the plane formed can be denoted by <script type="math/tex">\pi</script>. Since these points are coplanar, the rays back-projected from <script type="math/tex">\mathbf{x}</script> and <script type="math/tex">\mathbf{x'}</script> intersect at <script type="math/tex">\mathbf{X}</script>. This is the most significant property in searching for a correspondence.</p>

<div class="fig fighighlight">
  <img src="/assets/2019/p3/epipole1.png" width="120%" />
  <div class="figcaption">
 	Figure 2(a): Caption goes here.
  </div>
<br /><br />
  <img src="/assets/2019/p3/epipole2.png" width="120%" />
  <div class="figcaption">
  	Figure 2(b): Caption goes here.
  </div>
</div>

<p>Now, let us say that only <script type="math/tex">\mathbf{x}</script> is known, not <script type="math/tex">\mathbf{x'}</script>. We know that the point <script type="math/tex">\mathbf{x'}</script> lies in the plane <script type="math/tex">\pi</script> which is governed by the camera baseline <script type="math/tex">\mathbf{CC'}</script> and <script type="math/tex">\mathbf{\overrightarrow{Cx}}</script>.  Hence the point <script type="math/tex">\mathbf{x'}</script> lies on the line of intersetion of <script type="math/tex">\mathbf{l'}</script> of <script type="math/tex">\pi</script> with the second image plane. The line <script type="math/tex">\mathbf{l'}</script> is the image in the second view of the ray back-projected from <script type="math/tex">\mathbf{x}</script>. This line <script type="math/tex">\mathbf{l'}</script> is called the <em>epipolar line</em> corresponding to <script type="math/tex">\mathbf{x}</script>. The benifit is that you don’t need to search for the point corresponding to <script type="math/tex">\mathbf{x}</script> in the entire image plane as it can be restricted to the <script type="math/tex">\mathbf{l'}</script>.</p>

<ul>
  <li><strong>Epipole</strong> is the point of intersection of the line joining the camera centers with the image plane. (see <script type="math/tex">\mathbf{e}</script> and <script type="math/tex">\mathbf{e'}</script> in the Fig. 2(a))</li>
  <li><strong>Epipolar plane</strong> is the plane containing the baseline.</li>
  <li><strong>Epipolar line</strong> is the intersection of an epipolar plane with the image plane. <em>All the epipolar lines intersect at the epipole.</em></li>
</ul>

<p><a name="estfundmatrix"></a><br />
#### 3.2.2. The Fundamental Matrix <script type="math/tex">\mathbf{F}</script></p>

<p>The <script type="math/tex">\mathbf{F}</script> matrix is only an algebraic representation of epipolar geometry and can both geometrically <em>(contructing the epipolar line)</em> and arithematically. (<a href="http://cvrs.whu.edu.cn/downloads/ebooks/Multiple%20View%20Geometry%20in%20Computer%20Vision%20\(Second%20Edition\).pdf">See derivation</a>) (<a href="https://www.youtube.com/watch?v=DgGV3l82NTk">Fundamental Matrix Song</a>)<br />
As a result, we obtain:<br />
<script type="math/tex">\mathbf{x}_i'^{\ \mathbf{T}}\mathbf{F} \mathbf{x}_i = 0</script><br />
where <script type="math/tex">i=1,2,....,m.</script> <br />
This is known as epipolar constraint or correspondance condition (or <em>Longuet-Higgins</em> equation). Since, <script type="math/tex">\mathbf{F}</script> is a <script type="math/tex">3\times3</script> matrix, we can set up a homogenrous linear system with 9 unknowns:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{bmatrix} x'_i & y'_i & 1 \end{bmatrix}
\begin{bmatrix}f_{11} & f_{12} & f_{13} \\ f_{21} & f_{22} & f_{23} \\ f_{31} & f_{32} & f_{33} \end{bmatrix}
\begin{bmatrix} x_i \\ y_i \\ 1 \end{bmatrix} = 0 %]]></script>

<p><script type="math/tex">\begin{equation}x_i x'_i f_{11} + x_i y'_i f_{21} + x_i f_{31} + y_i x'_i f_{12} + y_i y'_i f_{22} + y_i f_{32} +  x'_i f_{13} + y'_i f_{23} + f_{33}=0\end{equation}</script><br />
<br /><br /></p>

<p>Simplifying for <script type="math/tex">m</script> correspondences,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{bmatrix} x_1 x'_1 & x_1 y'_1 & x_1 & y_1 x'_1 & y_1 y'_1 & y_1 &  x'_1 & y'_1 & 1 \\ \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\ x_m x'_m & x_m y'_m & x_m & y_m x'_m & y_m y'_m & y_m &  x'_m & y'_m & 1 \end{bmatrix}\begin{bmatrix} f_{11} \\ f_{21} \\ f_{31} \\ f_{12} \\ f_{22} \\ f_{32} \\ f_{13} \\f_{23} \\ f_{33}\end{bmatrix} = 0 %]]></script>

<p><strong><em>How many points do we need to solve the above equation? Think! Twice!</em></strong> <br />
Remember <em>homography</em>, where each point correspondence contributes two constraints? Unlike homography, in <script type="math/tex">\mathbf{F}</script> matrix estimation, each point only contributes one constraints as the epipolar constraint is a scalar equation.<br />
Thus, we require at least 8 points to solve the above homogenous system. That is why it is known as <a href="https://en.wikipedia.org/wiki/Eight-point_algorithm">Eight-point algorithm</a>.</p>

<p>With <script type="math/tex">N \geq 8</script> correspondences between two images, the fundamental matrix, <script type="math/tex">F</script> can be obtained as:<br />
By stacking the above equation in a matrix <script type="math/tex">A</script>, the equation<br />
<script type="math/tex">Ax=0</script> is obtained.<br />
	This system of equation can be answered by solving the linear least squares using Singular Value Decomposition (SVD) as explained in the <a href="https://cmsc426.github.io/math-tutorial/#svd">Math module</a>. When applying SVD to matrix <script type="math/tex">\mathbf{A}</script>, the decomposition <script type="math/tex">\mathbf{USV^T}</script> would be obtained with <script type="math/tex">\mathbf{U}</script> and <script type="math/tex">\mathbf{V}</script> orthonormal matrices and a diagonal matrix <script type="math/tex">\mathbf{S}</script> that contains the singular values. The singular values <script type="math/tex">\sigma_i</script> where <script type="math/tex">i\in[1,9], i\in\mathbb{Z}</script>, are positive and are in decreasing order with <script type="math/tex">\sigma_9=0</script> since we have 8 equations for 9 unknowns. Thus, the last column of <script type="math/tex">\mathbf{V}</script> is the true solution given that <script type="math/tex">\sigma_i\neq 0 \  \forall i\in[1,8], i\in\mathbb{Z}</script>. However, due to noise in the correspondences, the estimated <script type="math/tex">\mathbf{F}</script> matrix can be of rank 3 <em>i.e.</em> <script type="math/tex">\sigma_9\neq0</script>. So, to enfore the rank 2 constraint, the last singular value of the estimated <script type="math/tex">\mathbf{F}</script> must be set to zero. If <script type="math/tex">F</script> has a full rank then it will have an empty null-space <em>i.e.</em> it won’t have any point that is on entire set of lines. Thus, there wouldn’t be any epipoles. See Fig. 3 for full rank comparisons for <script type="math/tex">F</script> matrices.</p>

<div class="fig fighighlight">
  <img src="/assets/2019/p3/FMatrixRank.png" width="120%" />
  <div class="figcaption">
 	Figure 3: F Matrix: Rank 3 vs Rank 2 comparison
  </div>
  <div style="clear:both;"></div>
</div>

<p>In MATLAB, you can use <code class="highlighter-rouge">svd</code> to solve <script type="math/tex">\mathbf{x}</script> from <script type="math/tex">\mathbf{Ax}=0</script><br />
<code class="highlighter-rouge">
[U, S, V] = svd(A);
x = V(:, end);
F = reshape(x, [3,3])';
</code></p>

<p><strong>To sumarize, write a function <code class="highlighter-rouge">EstimateFundamentalMatrix.py</code> that linearly estimates a fundamental matrix <script type="math/tex">F</script>, such that <script type="math/tex">x_2^T F x_1 = 0</script>. The fundamental matrix can be estimated by solving the linear least squares <script type="math/tex">Ax=0</script>.</strong></p>

<p><a name="ransac"></a><br />
#### 3.2.3. Match Outlier Rejection via RANSAC</p>

<p>Since the point correspondences are computed using SIFT or some other feature descriptors, the data is bound to be noisy and (in general) contains several outliers. Thus, to remove these outliers, we use RANSAC algorithm <em>(Yes! The same as used in Panorama stitching!)</em> to obtain a better estimate of the fundamental matrix. So, out of all possibilities, the <script type="math/tex">\mathbf{F}</script> matrix with maximum number of inliers is chosen.<br />
Below is the pseduo-code that returns the <script type="math/tex">\mathbf{F}</script> matrix for a set of matching corresponding points (computed using SIFT) which maximizes the number of inliers.</p>

<div class="fig fighighlight">
  <img src="/assets/2019/p3/ransac.png" width="80%" />
  <div class="figcaption">
 	Algorithm 1: Get Inliers RANSAC
  </div>
  <div style="clear:both;"></div>
<br /><br />
  <img src="/assets/2019/p3/featmatchransac.png" width="100%" />
  <div class="figcaption">
 	Figure 4: Feature matching after RANSAC. (Green: Selected correspondences; Red: Rejected correspondences)
  </div>
  <div style="clear:both;"></div>
</div>

<p><strong>Given, <script type="math/tex">N\geq8</script> correspondences between two images, <script type="math/tex">x_1 \leftrightarrow x_2</script>, implement a function <code class="highlighter-rouge">GetInlierRANSANC.py</code> that estimates inlier correspondences using fundamental matrix based RANSAC.</strong></p>

<p><a name="estE"></a><br />
### 3.3. Estimate <em>Essential Matrix</em> from Fundamental Matrix</p>

<p>Since we have computed the <script type="math/tex">\mathbf{F}</script> using epipolar constrains, we can find the relative camera poses between the two images. This can be computed using the <em>Essential Matrix</em>, <script type="math/tex">\mathbf{E}</script>. Essential matrix is another <script type="math/tex">3\times3</script> matrix, but with some additional properties that relates the corresponding points assuming that the cameras obeys the pinhole model (unlike <script type="math/tex">\mathbf{F}</script>). More specifically, <br />
<script type="math/tex">\mathbf{E}</script> = <script type="math/tex">\mathbf{K^TFK}</script><br />
where <script type="math/tex">\mathbf{K}</script> is the camera calibration matrix or camera intrinsic matrix. Clearly, the essential matrix can be extracted from <script type="math/tex">\mathbf{F}</script> and <script type="math/tex">\mathbf{K}</script>. As in the case of <script type="math/tex">\mathbf{F}</script> matrix computation, the singular values of <script type="math/tex">\mathbf{E}</script> are not necessarily <script type="math/tex">(1,1,0)</script> due to the noise in <script type="math/tex">\mathbf{K}</script>. This can be corrected by reconstructing it with <script type="math/tex">(1,1,0)</script> singular values, <em>i.e.</em><br />
<script type="math/tex">% <![CDATA[
\mathbf{E}=U\begin{bmatrix}1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix}V^T %]]></script></p>

<p><em>It is important to note that the <script type="math/tex">\mathbf{F}</script> is defined in the original image space (i.e. pixel coordinates) whereas <script type="math/tex">\mathbf{E}</script> is in the normalized image coordinates. Normalized image coordinates have the origin at the optical center of the image. Also, relative camera poses between two views can be computed using <script type="math/tex">\mathbf{E}</script> matrix. Moreover, <script type="math/tex">\mathbf{F}</script> has 7 degrees of freedom while <script type="math/tex">\mathbf{E}</script> has 5 as it takes camera parameters in account. (<a href="http://users.cecs.anu.edu.au/~hongdong/new5pt_cameraREady_ver_1.pdf">5-Point Motion Estimation Made Easy</a>)</em></p>

<p><strong>Given <script type="math/tex">F</script> , estimate the essential matrix <script type="math/tex">E = K^T F K</script> by implementing the function <code class="highlighter-rouge">EssentialMatrixFromFundamentalMatrix.py</code>.</strong></p>

<p><a name="essential"></a><br />
### 3.4. Estimate <strong>Camera Pose</strong> from Essential Matrix<br />
The camera pose consists of 6 degrees-of-freedom (DOF) Rotation (Roll, Pitch, Yaw) and Translation (X, Y, Z) of the camera with respect to the world. Since the <script type="math/tex">\mathbf{E}</script> matrix is identified, the four camera pose configurations: <script type="math/tex">(C_1, R_1), (C_2, R_2), (C_3, R_3)</script> and <script type="math/tex">(C_4, R4)</script> where <script type="math/tex">\ C\in\mathbb{R}^3</script> is the camera center and <script type="math/tex">R\in SO(3)</script> is the rotation matrix, can be computed. Thus, the camera pose can be written as:<br />
<script type="math/tex">% <![CDATA[
P = KR\begin{bmatrix}I_{3\times3} & -C\end{bmatrix} %]]></script><br />
These four pose configurations can be computed from <script type="math/tex">\mathbf{E}</script> matrix. Let <script type="math/tex">\mathbf{E}=UDV^T</script> and <script type="math/tex">% <![CDATA[
W=\begin{bmatrix}0 & -1 & 0\\ 1 & 0 & 0\\ 0 & 0 & 1\end{bmatrix} %]]></script>. The four configurations can be written as: <br />
1. <script type="math/tex">C_1=U(:, 3)</script> and <script type="math/tex">R_1=UWV^T</script><br />
2. <script type="math/tex">C_2=-U(:, 3)</script> and <script type="math/tex">R_2=UWV^T</script><br />
3. <script type="math/tex">C_3=U(:, 3)</script> and <script type="math/tex">R_3=UW^TV^T</script><br />
4. <script type="math/tex">C_4=-U(:, 3)</script> and <script type="math/tex">R_4=UW^TV^T</script></p>

<p><strong>It is important to note that the <script type="math/tex">\ det(R)=1</script>. If <script type="math/tex">det(R)=-1</script>, the camera pose must be corrected <em>i.e.</em> <script type="math/tex">C=-C</script> and <script type="math/tex">R=-R</script>.</strong></p>

<p><strong>Implement the function <code class="highlighter-rouge">ExtractCameraPose.py</code>, given <script type="math/tex">E</script>.$$</strong></p>

<p><a name="tri"></a><br />
### 3.5. <strong>Triangulation</strong> Check for <strong>Cheirality Condition</strong> <br />
In the previous section, we computed four different possible camera poses for a pair of images using essential matrix. In this section we will triangulate the 3D points, given two camera poses.</p>

<p><strong>Given two camera poses, <script type="math/tex">(C_1, R_1)</script> and <script type="math/tex">(C_2, R_2)</script>, and correspondences, <script type="math/tex">x_1 \leftrightarrow x_2</script>, triangulate 3D points using linear least squares. Implement the function <code class="highlighter-rouge">LinearTriangulation.py</code>.</strong></p>

<p>Though, in order to find the <em>correct</em> unique camera pose, we need to remove the disambiguity. This can be accomplish by checking the <strong>cheirality condition</strong> <em>i.e.</em> <em>the reconstructed points must be in front of the cameras</em>. <br />
To check the cheirality condition, triangulate the 3D points (given two camera poses) using <strong>linear least squares</strong> to check the sign of the depth <script type="math/tex">Z</script> in the camera coordinate system w.r.t. camera center. A 3D point <script type="math/tex">X</script> is in front of the camera iff:<br />
<script type="math/tex">r_3\mathbf{(X-C)} > 0</script><br />
where <script type="math/tex">r_3</script> is the third row of the rotation matrix (z-axis of the camera). Not all triangulated points satisfy this coniditon due of the presence of correspondence noise. The best camera configuration, <script type="math/tex">(C, R, X)</script> is the one that produces the maximum number of points satisfying the cheirality condition.</p>

<div class="fig fighighlight">
  <img src="/assets/2019/p3/lintria.png" width="60%" />
  <div class="figcaption">
  	Figure 5: Initial triangulation plot with disambiguity, showing all four possible camera poses.
  </div>
  <div style="clear:both;"></div>
</div>

<p><strong>Given four camera pose configurations and their triangulated points, find the unique camera<br />
pose by checking the cheirality condition - the reconstructed points must be in front of the<br />
cameras (implement the function <code class="highlighter-rouge">DisambiguateCameraPose.py</code>).</strong></p>

<p><a name="nonlintri"></a><br />
#### 3.5.1. Non-Linear Triangulation<br />
Given two camera poses and linearly triangulated points, <script type="math/tex">X</script>, the locations of the 3D points that minimizes the reprojection error (Recall <a href="https://cmsc426.github.io/pano/#reproj">Project 2</a>) can be refined. The linear triangulation minimizes the algebraic error. Though, the reprojection error is geometrically meaningful error and can be computed by measuring error between measurement and projected 3D point:<br /><br />
<script type="math/tex">\underset{x}{\operatorname{min}}</script> <script type="math/tex">\sum_{j=1,2}\left(u^j - \frac{P_1^{jT}\widetilde{X}}{P_3^{jT}{X}}\right)^2 + \left(v^j - \frac{P_2^{jT}\widetilde{X}}{P_3^{jT}{X}}\right)^2</script></p>

<p>Here, <script type="math/tex">j</script> is the index of each camera, <script type="math/tex">\widetilde{X}</script> is the hoomogeneous representation of <script type="math/tex">X</script>. <script type="math/tex">P_i^T</script> is each row of camera projection matrix, <script type="math/tex">P</script>. This minimization is highly nonlinear due to the divisions. The initial guess of the solution, <script type="math/tex">X_0</script>, is estimated via the linear triangulation to minimize the cost function. This minimization can be solved using nonlinear optimization functions such as <code class="highlighter-rouge">scipy.optimize.leastsq</code> or <code class="highlighter-rouge">scipy.optimize.least_squares</code> in Scipy library.</p>

<div class="fig fighighlight">
  <img src="/assets/2019/p3/nonlintria.png" width="100%" />
  <div class="figcaption">
 	Figure 6: Comparison between non-linear vs linear triangulation.
  </div>
  <div style="clear:both;"></div>
</div>

<p><strong>Given two camera poses and linearly triangulated points, <script type="math/tex">X</script>, refine the locations of the 3D<br />
points that minimizes reprojection error (implement the function <code class="highlighter-rouge">NonlinearTriangulation.py</code>).</strong></p>

<p><a name="pnp"></a><br />
### 3.6. Perspective-<script type="math/tex">n</script>-Points</p>

<p>Now, since we have a set of <script type="math/tex">n</script> 3D points in the world, their <script type="math/tex">2D</script> projections in the image and the intrinsic parameter; the 6 DOF camera pose can be estimated using linear least squares. This fundamental problem, in general is known as <em>Persepective</em>-<script type="math/tex">n</script>- <em>Point</em> (PnP). For there to exist a solution, <script type="math/tex">n\geq 3</script>. There are multiple methods to solve the P<script type="math/tex">n</script>P problem and have an assumptions in most of them that the camera is calibrated. Methods such as <a href="https://pdfs.semanticscholar.org/f1d6/2775d4a51161663ff9453b37bb21a1263f25.pdf">Unified P<script type="math/tex">n</script>P</a> (or UPnP) do not abide with the said assumption as they estimate both intrinsic and extrinsic parameters. In this section, you will a simpler version of PnP. You will register a new image given 2D-3D correspondences, i.e. <script type="math/tex">X\leftrightarrow x</script> followed by nonlinear optimization.</p>

<h3 id="linear-camera-pose-estimation">3.6.1 Linear Camera Pose Estimation</h3>
<p>Given 2D-3D correspondences, <script type="math/tex">X\leftrightarrow x</script> and the intrinsic paramter <script type="math/tex">K</script>, estimate the camera pose using linear least squares (implement the function <code class="highlighter-rouge">LinearPnP.py</code>. 2D points can be normalized by the intrinsic parameter to isolate camera parameters, <script type="math/tex">(C,R)</script>, i.e. <script type="math/tex">K^{-1}x</script>. A linear least squares system that relates the 3D and 2D points can be solved for <script type="math/tex">(t, R)</script> where <script type="math/tex">t = −R^T C</script>. Since the linear least square solve does not enforce orthogonality of the rotation matrix, <script type="math/tex">R \in SO(3)</script>, the rotation matrix must be corrected by <script type="math/tex">R = UV^T</script> where <script type="math/tex">R=UDV^T</script>. If the corrected rotation has <script type="math/tex">-1</script> determinant, <script type="math/tex">R = −R</script>. This linear PnP requires at least 6 correspondences. <em>(Think why?)</em></p>

<div class="fig fighighlight">
  <img src="/assets/2019/p3/PnPRANSAC.png" width="50%" />
  <div class="figcaption">
 	Figure: Plot of the camera poses with feature points. Different color represents feature correspondences from different pair of images. Blue points are features from Image 1 and Image 2; Red points are features from Image 2 and Image 3 etc.
  </div>
  <div style="clear:both;"></div>
</div>

<h3 id="pnp-ransac">3.6.2 PnP RANSAC</h3>

<p>P<script type="math/tex">n</script>P is prone to error as there are outliers in the given set of point correspondences. To overcome this error, we can use RANSAC (yes, again!) to make our camera pose more robust to outliers. To formalize, given <script type="math/tex">N \geq 6</script> 3D-2D correspondences, <script type="math/tex">X \leftrightarrow x</script>, implement the following function that estimates camera pose <script type="math/tex">(C, R)</script> via RANSAC (implement the function PnPRANSAC.py).</p>

<p>The alogrithm below depicts the solution with RANSAC.</p>

<div class="fig fighighlight">
  <img src="/assets/2019/p3/pnpransac.png" width="80%" />
  <div class="figcaption">
 	Algorithm 2: PnP RANSAC
  </div>
  <div style="clear:both;"></div>
</div>

<p>Just like in triangulation, since we have the linearly estimated camera pose, we can refine the camera pose that minimizes the reprojection error (Linear PnP only minimizes the algebraic error).</p>

<h3 id="nonlinear-pnp">3.6.3 Nonlinear PnP</h3>
<p>Given <script type="math/tex">N \geq 6</script> 3D-2D correspondences, <script type="math/tex">X \leftrightarrow x</script>, and linearly estimated camera pose, <script type="math/tex">(C, R)</script>, refine the camera pose that minimizes reprojection error (implement the function <code class="highlighter-rouge">NonlinearPnP.py</code>). The linear PnP minimizes algebraic error. Reprojection error that is geometrically meaningful error is computed by measuring error between measurement and projected 3D point</p>

<script type="math/tex; mode=display">\underset{C,R}{\operatorname{min}} \sum_{i=1,J} \left(u^j - \frac{P_1^{jT}\widetilde{X_j}}{P_3^{jT}{\widetilde{X_j}}}\right)^2 + \left(v^j - \frac{P_2^{jT}\widetilde{X_j}}{P_3^{jT}{X_j}}\right)^2</script>

<p>here <script type="math/tex">\widetilde{X}</script> is the homogeneous representation of <script type="math/tex">X</script>. <script type="math/tex">P_i^T</script> is each row of camera projection matrix, <script type="math/tex">P</script> which is computed by <script type="math/tex">P = KR [I_{3\times3} − C]</script>. A compact representation of the rotation matrix using quaternion is a better choice to enforce orthogonality of the rotation matrix, <script type="math/tex">R = R(q)</script> where <script type="math/tex">q</script> is four dimensional quaternion, i.e.,</p>

<script type="math/tex; mode=display">\underset{C,q}{\operatorname{min}} \sum_{i=1,J} \left(u^j - \frac{P_1^{jT}\widetilde{X_j}}{P_3^{jT}{\widetilde{X_j}}}\right)^2 + \left(v^j - \frac{P_2^{jT}\widetilde{X_j}}{P_3^{jT}{X_j}}\right)^2</script>

<p>This minimization is highly nonlinear because of the divisions and quaternion parameteriza-<br />
tion. The initial guess of the solution, <script type="math/tex">(C_0, R_0)</script>, estimated via the linear PnP is needed to<br />
minimize the cost function. This minimization can be solved using a nonlinear optimization<br />
function such as <code class="highlighter-rouge">scipy.optimize.leastsq</code> or <code class="highlighter-rouge">scipy.optimize.least_squares</code> in Scipy library.</p>

<p><a name="ba"></a><br />
### 3.7. Bundle Adjustment</p>

<p>Once you have computed all the camera poses and 3D points, we need to refine the poses and 3D points together, initialized by previous reconstruction by minimizing reporjection error.</p>

<div class="fig fighighlight">
  <img src="/assets/2019/p3/BA.png" width="80%" />
  <div class="figcaption">
 	Figure 7: The final reconstructed scene after Sparse Bundle Adjustment (SBA).
  </div>
  <div style="clear:both;"></div>
</div>

<h3 id="visibility-matrix">3.7.1 Visibility Matrix</h3>

<p>Find the relationship between a camera and point, construct a <script type="math/tex">I \times J</script> binary matrix, <script type="math/tex">V</script> where<br />
<script type="math/tex">V_{ij}</script> is one if the <script type="math/tex">j^{th}</script> point is visible from the <script type="math/tex">i^{th}</script> camera and zero otherwise (implement the<br />
function <code class="highlighter-rouge">BuildVisibilityMatrix.py</code>)</p>

<h3 id="bundle-adjustment">3.7.2 Bundle Adjustment</h3>

<p>Given initialized camera poses and 3D points, refine them by minimizing reprojection error (implement the function <code class="highlighter-rouge">BundleAdjustment.py</code>). The bundle adjustment refines camera<br />
poses and 3D points simultaneously by minimizing the following reprojection error over <script type="math/tex">C_{i_{i=1}}^I</script>,<br />
<script type="math/tex">q_{i_{i=1}}^I</script> and <script type="math/tex">X_{j_{j=1}}^J</script>.</p>

<p>The optimization problem can formulated as following:</p>

<p><script type="math/tex">\underset{\{C_i, q_i\}_{i=1}^i,\{X\}_{j=1}^J}{\operatorname{min}}\sum_{i=1}^I\sum_{j=1}^J V_{ij}\left(\left(u^j - \dfrac{P_1^{jT}\tilde{X}}{P_3^{jT}\tilde{X}}\right)^2 + \left(v^j - \dfrac{P_2^{jT}\tilde{X}}{P_3^{jT}\tilde{X}}\right)^2\right)</script><br />
where <script type="math/tex">V_{ij}</script> is the visibility matrix.</p>

<p>Clearly, solving such a method to compute the structure from motion is complex and slow <em>(can take from several minutes for only 8-10 images)</em>. This minimization can be solved using a nonlinear optimization functions such as <code class="highlighter-rouge">scipy.optimize.leastsq</code> but will be extremely slow due to a number of parameters. The Sparse Bundle Adjustment toolbox such as <a href="https://buildmedia.readthedocs.org/media/pdf/python-sba/latest/python-sba.pdf">pySBA</a> and <a href="https://scipy-cookbook.readthedocs.io/items/bundle_adjustment.html">large-scale BA in scipy</a> are designed to solve such optimization by exploiting sparsity of visibility matrix, <script type="math/tex">V</script> . Note that a small number of entries in <script type="math/tex">V</script> are one because a 3D point is visible from a small subset of images. Using the sparse bundle adjustment package is not trivial and would be much faster than one you write. For SBA, you are allowed to use any optimization library.</p>

<h2 id="putting-the-pipeline-together">4. Putting the pipeline together</h2>

<p>Write a program <code class="highlighter-rouge">Wrapper.py</code> that run the full pipeline of structure from motion based on the above algorithm.</p>

<p>Also, compare your result against VSfM output. You can download the off-the-shelf SfM software here: <a href="http://ccwu.me/vsfm/">VSfM</a>.</p>

<h2 id="project-overview">Project Overview</h2>

<div class="fig fighighlight">
  <img src="/assets/2019/p3/summary.png" width="80%" />
  <div class="figcaption">
 	Figure: The overview.
  </div>
  <div style="clear:both;"></div>
</div>

<p><a name="testset"></a><br />
## 5. Notes about the Data Set<br />
Run your SfM algorithm on the images provided <a href="https://github.com/cmsc733/cmsc733.github.io/blob/master/assets/2019/p3/Data.zip">here</a>. The data given to you are a set of 6 images of building in-front of Levine Hall at UPenn, using a GoPro Hero 3 with fisheye lens distortion corrected. Keypoints matching (SIFT keypoints and descriptors used) data is also provided in the same folder for pairs of images. The data folder contains 5 matching files named <code class="highlighter-rouge">matching*.txt</code> where <code class="highlighter-rouge">*</code> refers to numbers from 1 to 5. For eg., <code class="highlighter-rouge">matching3.txt</code> contains the matching between the third image and the fourth, fifth and sixth images, i.e., <script type="math/tex">\mathcal{I}_3 \leftrightarrow \mathcal{I}_4</script>, <script type="math/tex">\mathcal{I}_3 \leftrightarrow \mathcal{I}_5</script> and <script type="math/tex">\mathcal{I}_3 \leftrightarrow \mathcal{I}_6</script> . Therefore, <code class="highlighter-rouge">matching6.txt</code> does not exist because it is the matching by itself.</p>

<p>The file format of the matching file is described next. Each matching file is formatted as<br />
follows for the i th matching file:</p>

<p><strong>nFeatures:</strong> (the number of feature points of the <script type="math/tex">i^{th}</script> image - each following row specifies<br />
matches across images given a feature location in the <script type="math/tex">i^{th}</script> image.)</p>

<p><strong>Each Row:</strong> (the number of matches for the <script type="math/tex">j^{th}</script> feature) (Red Value) (Green Value) (Blue Value) (<script type="math/tex">u_{\texttt{current image}}</script>) (<script type="math/tex">v_{\texttt{current image}}</script>) (image id) (<script type="math/tex">u_{\texttt{image id image}}</script>) (<script type="math/tex">v_{\texttt{image id image}}</script>) (image id) (<script type="math/tex">u_{\texttt{image id image}}</script>)<br />
(v_{image id image}) …</p>

<p>An example of matching1.txt is given below:<br />
<code class="highlighter-rouge">
nFeatures: 2002
3 137 128 105 454.740000 392.370000 2 308.570000 500.320000 4 447.580000 479.360000
2 137 128 105 454.740000 392.370000 4 447.580000 479.360000
</code><br />
The images are taken at 1280 × 960 resolution and the camera intrinsic parameters <script type="math/tex">K</script> are given in <code class="highlighter-rouge">calibration.txt</code> file. You will program this full pipeline guided by the functions described in following sections.</p>

<p><strong>For the extra credit:</strong><br />
Also, capture a set of images and run your SfM algorithm. DO NOT steal images from the internet. Analyze the success and the failure of your algorithm and showcase that in your report. Note: You need to capture images, calibrate them and undistort them. Feel free to use any in-built calibration tool for this. MATLAB’s calibration tool in Computer Vision toolbox will be handy.</p>

<p><a name="sub"></a><br />
## 6. Submission Guidelines</p>

<p><b> If your submission does not comply with the following guidelines, you’ll be given ZERO credit </b></p>

<p><a name="files"></a><br />
### 6.1. File tree and naming</p>

<p>Your submission on ELMS/Canvas must be a <code class="highlighter-rouge">zip</code> file, following the naming convention <code class="highlighter-rouge">YourDirectoryID_p3.zip</code>. If you email ID is <code class="highlighter-rouge">abc@umd.edu</code> or <code class="highlighter-rouge">abc@terpmail.umd.edu</code>, then your <code class="highlighter-rouge">DirectoryID</code> is <code class="highlighter-rouge">abc</code>. For our example, the submission file should be named <code class="highlighter-rouge">abc_p1.zip</code>. The file <strong>must have the following directory structure</strong> because we’ll be autograding assignments. The file to run for your project should be called <code class="highlighter-rouge">Wrapper.py</code>. You can have any helper functions in sub-folders as you wish, be sure to index them using relative paths and if you have command line arguments for your Wrapper codes, make sure to have default values too. Please provide detailed instructions on how to run your code in <code class="highlighter-rouge">README.md</code> file. Please <strong>DO NOT</strong> include data in your submission.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>YourDirectoryID_hw1.zip
│   README.md
|   Your Code files 
|   ├── GetInliersRANSAC.py
|   ├── EstimateFundamentalMatrix.py
|   ├── EssentialMatrixFromFundamentalMatrix.py
|   ├── ExtractCameraPose.py
|   ├── LinearTriangulation.py
|   ├── DisambiguateCameraPose.py
|   ├── NonlinearTriangulation.py
|   ├── PnPRANSAC.py
|   ├── NonlinearPnP.py
|   ├── BuildVisibilityMatrix.py
|   ├── BundleAdjustment.py
|   ├── Wrapper.py
|   ├── Any subfolders you want along with files
|   Wrapper.py 
|   Data
|   ├── BundleAdjustmentOutputForAllImage
|   ├── FeatureCorrespondenceOutputForAllImageSet
|   ├── LinearTriangulationOutputForAllImageSet
|   ├── NonLinearTriangulationOutputForAllImageSet
|   ├── PnPOutputForAllImageSetShowingCameraPoses
|   ├── Imgs/
└── Report.pdf
</code></pre>
</div>
<p><a name="report"></a><br />
### 6.2. Report</p>

<p>There will be no Test Set for this project. <br />
For each section of the project, explain briefly what you did, and describe any interesting problems you encountered and/or solutions you implemented.  You must include the following details in your writeup:</p>

<ul>
  <li>
    <p>Please make your report extremely detailed with re-projection error after each step (Linear, Non-linear triangulation, Linear, Non-linear PnP before and after BA and so on). Describe all the steps (anything that is not obvious) and any<br />
other observations in your report.</p>
  </li>
  <li>
    <p>Your report <strong>MUST</strong> be typeset in LaTeX in the IEEE Tran format provided to you in the <code class="highlighter-rouge">Draft</code> folder and should of a conference quality paper.</p>
  </li>
  <li>
    <p>Present the Data you collected in <code class="highlighter-rouge">Data/Imgs/</code>.</p>
  </li>
  <li>
    <p>Present failure cases and explanation, if any.</p>
  </li>
  <li>
    <p>Do not use any function that directly implements a part of the pipeline. If you have any doubts, please contact us via Piazza.</p>
  </li>
</ul>

<p><a name="coll"></a><br />
## 7. Collaboration Policy<br />
You are encouraged to discuss the ideas with your peers. However, the code should be your own, and should be the result of you exercising your own understanding of it. If you reference anyone else’s code in writing your project, you must properly cite it in your code (in comments) and your writeup. For the full honor code refer to the CMSC733 Spring 2019 website.</p>

  </article>

</div>
      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <div class="footer-col-1 column">
      <ul>
        
        
        
        <li>
          <a href="mailto:"></a>
        </li>
      </ul>
    </div>

    <div class="footer-col-2 column">
        
    </div>

    <div class="footer-col-3 column">
      
    </div>

  </div>

</footer>


    <!-- mathjax -->
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </body>
</html>
