<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Learning the basics of Computer Vision</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="Course materials and notes for University of Maryland's class CMSC733: Computer Vision.">
    <link rel="canonical" href="http://cmsc733.github.io/pano-prereq/">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">

    <!-- Google fonts -->
    <link href='http://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>

    <!-- Google tracking -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-46895817-2', 'auto');
      ga('send', 'pageview');

    </script>
    
</head>


    <body>

    <header class="site-header">

  <div class="wrap title-wrap">
    <a class="site-title" href="/">CMSC733 Computer Vision</a>
  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Learning the basics of Computer Vision</h1>
  </header>

  <article class="post-content">
  <p><strong>This article is written by <a href="http://chahatdeep.github.io/">Chahat Deep Singh.</a></strong> (chahat[at]terpmail.umd.edu)</p>

<p>Please contact <strong>Chahat</strong> if there are any errors.</p>

<p>Table of Contents:</p>

<ul>
  <li><a href="#intro">Introduction</a></li>
  <li><a href="#feat-conv">Features and Convolution</a>
    <ul>
      <li><a href="#feat">Features</a></li>
      <li><a href="#conv">Convolution</a></li>
      <li><a href="#deconv">Deconvolution</a></li>
      <li><a href="#diff-operators">Different Operators</a></li>
      <li><a href="#corner-detection">Corner Detection</a><br />
<!-- - [Optical Flow](#flow) (Coming Soon) --></li>
    </ul>
  </li>
</ul>

<p><a name="intro"></a><br />
## 1. Introduction</p>

<p>We have seen fascinating things that our camera applications like instagram, snachat or your default phone app do. It can be creating a full structure of your face for facial recognition or simply creating a panorama from multiple images. In this course, we will learn how to recreate such applications but before that we require to learn about the basics like filtering, features, camera models and transformations. This article is all about laying down the groundwork for the aforementioned applications. Let’s start with understanding the most basic aspects of an image: features.</p>

<p><a name="feat-conv"></a><br />
## 2. Features and Convolution<br />
<a name="feature"></a><br />
### 2.1 What are features?<br />
What are <i>features</i> in Machine vision? <i> Is it similar as Human visual perception?</i> This question can have different answers but one thing is certain that feature detection is an imperative building block for a variety of computer vision applications. We all have seen <i> Panorama Stitching </i> in our smartphones or other softwares like Adobe Photoshop or AutoPano Pro. The fundamental idea in such softwares is to align two or more images before seamlessly stitching into a panorama image. Now back to the question, <i> what kind of features should be detected before the alignment? </i> Can you think of a few types of features?</p>

<p>Certain locations in the images like corners of a building or mountain peaks can be considered as features. These kinds of localized features are known as <i>corners</i> or keypoints or interest points and are widely used in different applications. These are characterized by the appearance of neigborhood pixels surrounding the point (or local patches). Fig. 1 demonstrates strong and weak corner features.</p>
<div class="fig figcenter fighighlight">
  <img src="/assets/pano/strong-corners.png" width="49%" />
  <div class="figcaption">Fig. 1: The section in <b><font color="red">red</font></b> illustrates good or strong corner features while the <b>black</b> depicts weak or bad features.</div>
</div>

<p>The other kind of feature is based on the orientation and local appearance and is generally a good indicator of object boundaries and occlusion events. <i> (Occlusion means that there is something in the field of view of the camera but due to some sensor/optical property or some other scenario, you can’t.)</i> There are multiple ways to detect certain features. One of the way is <b>convolution</b>.</p>

<p><a name="conv"></a><br />
### 2.2 Convolution<br />
<i>Ever heard of Convolutional Neural Networks (CNN)? What is convolution? Is it really that ‘convoluted’? </i> Let’s try to answer such questions but before that let’s understand what convolution really means! Think of it as an operation of changing the pixel values to a new set of values based on the values of the nearby pixels. <i> Didn’t get the gist of it? Don’t worry! </i></p>

<p>Convolution is an operation between two functions, resulting in another function that depicts how the shape of first function is modified by the second function. The convolution of two functions, say <script type="math/tex">f</script> and <script type="math/tex">g</script> is written is <script type="math/tex">f\star g</script> or <script type="math/tex">f*g</script> and is defined as:</p>

<script type="math/tex; mode=display">(f*g)(t) = \int_{-\infty}^{\infty} f(\tau)g(t-\tau)d\tau = \int_{-\infty}^{\infty} f(t-\tau)g(\tau)d\tau</script>

<p>Let’s try to visualize convolution in one dimension. The following <i>figure</i> depcits the convolution <i>(in black)</i> of the two functions <i>(blue and red).</i> One can think convolution as the common area under the functions  <script type="math/tex">f</script> and <script type="math/tex">g</script>.</p>
<div class="fig figcenter fighighlight">
  <img src="/assets/pano/conv.gif" width="49%" />
  <div class="figcaption">Convolution of \(f\) and \(g\) is given by the area under the black curve.</div>
</div>

<p>Since we would be dealing with discrete functions in this course (as images are of the size <script type="math/tex">M\times N</script>), let us look at a simple discrete 1D example:<br />
<script type="math/tex">f = [10, 50, 60, 10, 20, 40, 30]</script> and <br />
<script type="math/tex">g = [1/3, 1/3, 1/3]</script>.<br />
Let the output be denoted by <script type="math/tex">h</script>. What would be the value of <script type="math/tex">h(3)</script>? In order to compute this, we slide <script type="math/tex">g</script> so that it is centered around <script type="math/tex">f(3)</script> <em>i.e.</em><br />
<script type="math/tex">% <![CDATA[
\begin{bmatrix}10 & 50 & 60 & 10 & 20 & 40 & 30\\0 & 1/3 & 1/3 & 1/3 & 0 & 0 & 0\end{bmatrix} %]]></script>.<br />
We multiply the corresponding values of <script type="math/tex">f</script> and <script type="math/tex">g</script> and then add up the products <em>i.e.</em><br />
<script type="math/tex">h(3)=\dfrac{1}{3}50+\dfrac{1}{3}60\dfrac{1}{3}10=40</script><br />
It can be inferred that the function <script type="math/tex">g</script> (also known as kernel or the filter) is computing a windowed average of the image.</p>

<p>Similarly, one can compute 2D convolutions (and hence any <script type="math/tex">N</script> dimensions convolution) as shown in image below:</p>
<div class="fig figcenter fighighlight">
  <img src="/assets/pano/Conv2D.png" width="49%" />
  <div class="figcaption">Convolution of a Matrix with a kernal.</div>
</div>
<p>These convolutions are the most commonly used operations for smoothing and sharpening tasks. Look at the example down below:</p>
<div class="fig figcenter fighighlight">
  <img src="/assets/pano/ConvImg.png" width="49%" />
  <div class="figcaption">Smoothing/Blurring using Convolution.</div>
</div>
<p>Different kernels (or convolution masks) can be used to perform different level of sharpness:</p>

<div class="fig figcenter fighighlight">
  <img src="/assets/pano/DifferentKernel.png" width="49%" />
  <div class="figcaption">(a). Convolution with an identity masks results the same image as the input image. (b). Sharpening the image. (c). Normalization (or box blur). (d). 3X3 Gaussian blur. (e). 5X5 Gaussian blur. (f). 5X5 unsharp mask, it is based on the gaussian blur. NOTE: The denominator outside all the matrices are used to normalize the operation.</div>
</div>

<p>Apart from the smoothness operations, convolutions can be used to detect features such as edges as well. The figure given below shows <br />
how different kernel can be used to find the edges in an image using convolution.</p>
<div class="fig figcenter fighighlight">
  <img src="/assets/pano/ConvImgEdge.png" width="49%" />
  <div class="figcaption">Detecting edges in an image with different kernels.</div>
</div>

<p>A good explanation of convolution can also be found <a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/">here</a>.</p>

<p><a name="deconv"></a><br />
### Deconvolution:</p>

<p>Clearly as the name suggests, deconvolution is simply a process that reverses the effects of convolution on the given information. Deconvolution is implemented (generally) by computing the <em>Fourier Transform</em> of the signal <script type="math/tex">h</script> and the transfer function <script type="math/tex">g</script> (where <script type="math/tex">h=f * g</script>). In frequency domain, (assuming no noise) we can say that: <script type="math/tex">F=H/G</script>. Fourier Transformation: <a href="https://betterexplained.com/articles/an-interactive-guide-to-the-fourier-transform/">Optional Read</a>, <a href="https://www.youtube.com/watch?v=spUNpyF58BY/">Video</a></p>

<p>One can perform deblurring and restoration tasks using deconvolution as shown in figure:</p>
<div class="fig figcenter fighighlight">
  <img src="/assets/pano/deconv.jpg" width="49%" />
  <div class="figcaption">Left half of the image represents the input image. Right half represents the image after deconvolution.</div>
</div>

<p>Now, since we have learned the fundamentals about convolution and deconvolution, let’s dig deep into <em>Kernels</em> or <em>point operators</em>). One can apply small convolution filters of size <script type="math/tex">2\times2</script> or <script type="math/tex">3\times3</script> or so on. These can be <em>Sobel, Roberts, Prewitt, Laplacian</em> operators etc. We’ll learn about them in a while. Operators like these are a good approximation of the derivates in an image. While for a better texture analysis in an image, larger masks like <em>Gabor</em> filters are used.<br />
But what does it mean to take the derivative of an image? The derivative or the gradient of an image is defined as: <br />
<script type="math/tex">\nabla f=\left[\dfrac{\delta f}{\delta x}, \dfrac{\delta f}{\delta y}\right]</script><br />
It is important to note that the gradient of an image points towards the direction in which the intensity changes at the highest rate and thus the direction is given by:<br />
<script type="math/tex">\theta=tan^{-1}\left(\dfrac{\delta f}{\delta y}\Bigg{/}\dfrac{\delta f}{\delta x}\right)</script><br />
Moreover, the gradient direction is always perpendicular to the edge and the edge strength can by given by:<br />
<script type="math/tex">||\nabla f|| = \sqrt{\left(\dfrac{\delta f}{\delta x}\right)^2 + \left(\dfrac{\delta f}{\delta y}\right)^2}</script>. In practice, the partial derivatives can be written (in discrete form) as the different of values between consecutive pixels <em>i.e.</em> <script type="math/tex">f(x+1,y) -  f(x,y)</script> as <script type="math/tex">\delta x</script>  and  <script type="math/tex">f(x,y+1) -  f(x,y)</script> as <script type="math/tex">\delta y</script>.</p>

<p>Figure below shows commonly used gradient operators.</p>
<div class="fig figcenter fighighlight">
  <img src="/assets/pano/gradoperators.png" width="49%" />
  <div class="figcaption">Commonly used gradient operators.</div>
</div>

<p>You can implement the following in MATLAB using the function <code class="highlighter-rouge">edge</code> with various methods for edge detection. <br />
Before reading any further, try the following in MATLAB:<br />
-  <code class="highlighter-rouge">C = conv2(A, B)</code> <b><a href="https://www.mathworks.com/help/matlab/ref/conv2.html">Refer</a> </b><br />
-  <code class="highlighter-rouge">BW = edge(I, method, threshold, sigma)</code> <b><a href="https://www.mathworks.com/help/images/ref/edge.html">Refer</a></b><br />
-  <code class="highlighter-rouge">B = imfilter(A, h, 'conv')</code> <b><a href="https://www.mathworks.com/help/images/ref/imfilter.html">Refer</a></b></p>

<p><a name="diff-operators"></a><br />
### Different Operators:</p>

<h4 id="sobel-operator">Sobel Operator:</h4>
<p>This operator has two <script type="math/tex">3\times 3</script> kernels that are convolved with the original image <code class="highlighter-rouge">I</code> in order to compute the approximations of the derivatives.<br />
The horizontal and vertical are defined as follows:</p>

<script type="math/tex; mode=display">% <![CDATA[
G_x = \begin{bmatrix} +1 & 0 & -1 \\ +2 & 0 & -2 \\ +1 & 0 & -1 \end{bmatrix} %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
G_y = \begin{bmatrix} +1 & +2 & +1 \\ 0 & 0 & 0 \\ -1 & -2 & -1 \end{bmatrix} %]]></script>

<p>The gradient magnitude can be given as: <script type="math/tex">G=\sqrt{G^2_x + G^2_y}</script> and the gradient direction can be written as: <script type="math/tex">\theta=a tan \Bigg(\cfrac{G_y}{G_x}\Bigg)</script>. Figure below shows the input image and its output after convolving with Sobel operator.</p>

<div class="fig figcenter fighighlight">
  <img src="/assets/pano/filt1.png" width="70%" />
  <div class="figcaption">(a). Input Image. (b). Sobel Output.</div>
</div>

<h4 id="prewitt">Prewitt:</h4>

<script type="math/tex; mode=display">% <![CDATA[
G_x = \begin{bmatrix} +1 & 0 & -1 \\ +1 & 0 & -1 \\ +1 & 0 & -1 \end{bmatrix} %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
G_y = \begin{bmatrix} +1 & +1 & +1 \\ 0 & 0 & 0 \\ -1 & -1 & -1 \end{bmatrix} %]]></script>

<h4 id="roberts">Roberts:</h4>

<script type="math/tex; mode=display">% <![CDATA[
G_x = \begin{bmatrix} +1 & 0 \\ 0 & -1 \end{bmatrix} %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
G_x = \begin{bmatrix} 0 & +1 \\ -1 & 0 \end{bmatrix} %]]></script>

<h4 id="canny">Canny:</h4>
<p>Unlike any other filters, we have studied above, canny goes a bit further. In Canny edge detection, before finding the intensities of the image, a gaussian filter is applied to smooth the image in order to remove the noise. Now, one the gradient intensity is computed on the imge, it uses <em>non-maximum suppression</em> to suppress only the weaker edges in the image. Refer: <a href="https://ieeexplore.ieee.org/document/4767851/">A computational approach to edge detection</a>. <code class="highlighter-rouge">MATLAB</code> has an <code class="highlighter-rouge">approxcanny</code> function that finds the edges using an approximate version which provides faster execution time at the expense of less precise detection. Figure below illustrates different edge detectors.</p>

<div class="fig figcenter fighighlight">
  <img src="/assets/pano/filt2.png" width="80%" />
  <div class="figcaption">(a). Prewitt Output. (b). Roberts Output. (c). Canny Output (d) Laplacian of Gaussian (LoG). </div>
</div>

<p><a name="corner-detection"></a><br />
### Corner Detection:<br />
Now that we have learned about different edge features, let’s understand what corner features are! Corner detection is used to extract certain kinds of features and common in panorama stitching (Project 1), video tracking (project 3), object recognition, motion detection etc.</p>

<p><em>What is a corner?</em> To put it simply, a corner is the intersection of two edges. One can also define corner as: if there exist a point in an image such that there are two defintive (and different) edge directions in a local neighborhood of that point, then it is considered as a corner feature. In computer vision, corners are commonly written as ‘interest points’ in literature.</p>

<p>The paramount property of a corner detector is the ability to detect the same corner in multiple images under different translation, rotation, lighting etc. The simplest approach for corner detection in images is using correlation. (Correlation is similar in nature to convolution of two functions). Optional Read: <a href="http://www.ee.ic.ac.uk/hp/staff/dmb/courses/e1fourier/00800_correlation.pdf">Correlation</a></p>

<p>In MATLAB, try the following:<br />
- <code class="highlighter-rouge">corner(I, 'detectHarrisFeatures')</code>: Harris Corners<br />
- <code class="highlighter-rouge">corner(I, 'detectMinEigenFeatures')</code>: Shi-Tomasi Corners <br />
- <code class="highlighter-rouge">detectFASTFeatures(I)</code>: FAST: Features from Accelerated Segment Test</p>

<div class="fig figcenter fighighlight">
  <img src="/assets/pano/harris.jpg" width="49%" />
  <div class="figcaption">Harris Corner Detection.</div>
</div>

<!--
<a name='blob-detection'></a>
### Blob detection:
Another important element in computer vision is blob. _What is a blob?_ A blob is a group of connected pixels in an image that shares some common properties like intensity values. In other words, all the pixels in a blob can be considered to be similiar to each other. Convolution is the most common method for blob detection. 

- LoG (Laplacian of Gaussian):
One of the most common blob detectors uses LoG. Laplacian filters are derivative filters used to find areas of 
Let $$f(x,y)$$ be the input image. Let $$g(x,y,t)$$ be the Gaussian kernel:

$$g(x,y,t)=\cfrac{1}{2\pi t} e^{-\frac{x^2+y^2}{2t}}$$

where 

<div class="fig figcenter fighighlight">
  <img src="/assets/pano/blob1.jpg" width="49%">
  <img src="/assets/pano/blob.png" width="49%">
  <div class="figcaption">Harris Corner Detection.</div>
</div>



- DoG (Difference of Gaussian)


<a name='feat-desc'></a>
### Feature Descriptor:

- Feature Descriptor: SIFT (Scale selectivity (CIS 580) Multi-scale concepts), SURF and HOG

-->

  </article>

</div>
      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <div class="footer-col-1 column">
      <ul>
        
        
        
        <li>
          <a href="mailto:"></a>
        </li>
      </ul>
    </div>

    <div class="footer-col-2 column">
        
    </div>

    <div class="footer-col-3 column">
      
    </div>

  </div>

</footer>


    <!-- mathjax -->
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </body>
</html>
